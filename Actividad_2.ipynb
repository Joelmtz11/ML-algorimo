{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1jQ05FRijZxj"
      },
      "outputs": [],
      "source": [
        "import numpy as np #Linear algebra and mathematical operations\n",
        "import pandas as pd #importing and loading data\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris_df = pd.read_csv(\"Iris.csv\")\n",
        "iris_df = iris_df.sample(frac=1).reset_index(drop=True) # Shuffle"
      ],
      "metadata": {
        "id": "i7BAFaf0jci_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "iris_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "B4oFtbfbkaFt",
        "outputId": "0326aa8d-4a8d-4b16-c234-acb0465bb0a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
              "0   65            5.6           2.9            3.6           1.3   \n",
              "1   90            5.5           2.5            4.0           1.3   \n",
              "2  123            7.7           2.8            6.7           2.0   \n",
              "3  149            6.2           3.4            5.4           2.3   \n",
              "4  147            6.3           2.5            5.0           1.9   \n",
              "\n",
              "           Species  \n",
              "0  Iris-versicolor  \n",
              "1  Iris-versicolor  \n",
              "2   Iris-virginica  \n",
              "3   Iris-virginica  \n",
              "4   Iris-virginica  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-faa99ebd-80e7-4ab2-901e-1d862c73740a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>Species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>65</td>\n",
              "      <td>5.6</td>\n",
              "      <td>2.9</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.3</td>\n",
              "      <td>Iris-versicolor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90</td>\n",
              "      <td>5.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>Iris-versicolor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>123</td>\n",
              "      <td>7.7</td>\n",
              "      <td>2.8</td>\n",
              "      <td>6.7</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>149</td>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>147</td>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faa99ebd-80e7-4ab2-901e-1d862c73740a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-faa99ebd-80e7-4ab2-901e-1d862c73740a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-faa99ebd-80e7-4ab2-901e-1d862c73740a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-49536054-891c-44b7-96c8-4c5b00f628e9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-49536054-891c-44b7-96c8-4c5b00f628e9')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-49536054-891c-44b7-96c8-4c5b00f628e9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = iris_df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\n",
        "X = np.array(X)\n",
        "X[:5]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjFWk5an1HOn",
        "outputId": "b9f054ff-f911-4003-d055-15977390aa39"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.6, 2.9, 3.6, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "Y = iris_df.Species\n",
        "Y = one_hot_encoder.fit_transform(np.array(Y).reshape(-1, 1))\n",
        "Y[:5]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm0DKT0m1Isx",
        "outputId": "66597051-461d-4a1f-a68c-ff7a6bc416dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation function (sigmoid)\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Mean squared error loss\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "# Neural network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_dim, output_dim, nodes, learning_rate):\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.layers = len(nodes)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        prev_nodes = input_dim\n",
        "        for num_nodes in nodes:\n",
        "            self.weights.append(np.random.rand(prev_nodes, num_nodes))\n",
        "            self.biases.append(np.zeros((1, num_nodes)))\n",
        "            prev_nodes = num_nodes\n",
        "\n",
        "        self.weights.append(np.random.rand(prev_nodes, output_dim))\n",
        "        self.biases.append(np.zeros((1, output_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.layer_outputs = []\n",
        "        self.activations = [x]\n",
        "\n",
        "        for i in range(self.layers + 1):\n",
        "            output = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.layer_outputs.append(output)\n",
        "            activation = sigmoid(output)\n",
        "            self.activations.append(activation)\n",
        "\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def backward(self, x, y_true):\n",
        "        output_error = y_true - self.activations[-1]\n",
        "        for i in range(self.layers, -1, -1):\n",
        "            delta = output_error * sigmoid_derivative(self.activations[i + 1])\n",
        "            self.weights[i] += self.learning_rate * np.dot(self.activations[i].T, delta)\n",
        "            self.biases[i] += self.learning_rate * delta\n",
        "            output_error = np.dot(delta, self.weights[i].T)\n",
        "\n",
        "    def train(self, X_train, Y_train, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            for x, y_true in zip(X_train, Y_train):\n",
        "                x = np.array(x).reshape(1, -1)\n",
        "                y_true = np.array(y_true).reshape(1, -1)\n",
        "                y_pred = self.forward(x)\n",
        "                self.backward(x, y_true)\n",
        "\n",
        "                if epoch % 20 == 0:\n",
        "                    loss = mse_loss(y_true, y_pred)\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Define your data and parameters\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1)\n",
        "\n",
        "f = len(X_train[0])  # Number of features\n",
        "o = len(Y_train[0])  # Number of classes\n",
        "layers = [5, 10]     # Number of nodes in hidden layers\n",
        "L, E = 0.15, 100     # Learning rate and epochs\n",
        "\n",
        "# Create an instance of the NeuralNetwork class\n",
        "model = NeuralNetwork(input_dim=f, output_dim=o, nodes=layers, learning_rate=L)\n",
        "\n",
        "# Train the model\n",
        "model.train(X_train, Y_train, epochs=E)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsE4TwCNdWYb",
        "outputId": "6916085c-6995-4caa-c780-ede1143861b3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6465\n",
            "Epoch 0, Loss: 0.6483\n",
            "Epoch 0, Loss: 0.6457\n",
            "Epoch 0, Loss: 0.6363\n",
            "Epoch 0, Loss: 0.6479\n",
            "Epoch 0, Loss: 0.6448\n",
            "Epoch 0, Loss: 0.6442\n",
            "Epoch 0, Loss: 0.6436\n",
            "Epoch 0, Loss: 0.6473\n",
            "Epoch 0, Loss: 0.6327\n",
            "Epoch 0, Loss: 0.6463\n",
            "Epoch 0, Loss: 0.6315\n",
            "Epoch 0, Loss: 0.6301\n",
            "Epoch 0, Loss: 0.6290\n",
            "Epoch 0, Loss: 0.6401\n",
            "Epoch 0, Loss: 0.6266\n",
            "Epoch 0, Loss: 0.6250\n",
            "Epoch 0, Loss: 0.6434\n",
            "Epoch 0, Loss: 0.6225\n",
            "Epoch 0, Loss: 0.6213\n",
            "Epoch 0, Loss: 0.6412\n",
            "Epoch 0, Loss: 0.6180\n",
            "Epoch 0, Loss: 0.6335\n",
            "Epoch 0, Loss: 0.6399\n",
            "Epoch 0, Loss: 0.6387\n",
            "Epoch 0, Loss: 0.6135\n",
            "Epoch 0, Loss: 0.6370\n",
            "Epoch 0, Loss: 0.6092\n",
            "Epoch 0, Loss: 0.6064\n",
            "Epoch 0, Loss: 0.6332\n",
            "Epoch 0, Loss: 0.6269\n",
            "Epoch 0, Loss: 0.6000\n",
            "Epoch 0, Loss: 0.6306\n",
            "Epoch 0, Loss: 0.6285\n",
            "Epoch 0, Loss: 0.6226\n",
            "Epoch 0, Loss: 0.6266\n",
            "Epoch 0, Loss: 0.6248\n",
            "Epoch 0, Loss: 0.6223\n",
            "Epoch 0, Loss: 0.6209\n",
            "Epoch 0, Loss: 0.6180\n",
            "Epoch 0, Loss: 0.5777\n",
            "Epoch 0, Loss: 0.6172\n",
            "Epoch 0, Loss: 0.6120\n",
            "Epoch 0, Loss: 0.5656\n",
            "Epoch 0, Loss: 0.5559\n",
            "Epoch 0, Loss: 0.5473\n",
            "Epoch 0, Loss: 0.5939\n",
            "Epoch 0, Loss: 0.5296\n",
            "Epoch 0, Loss: 0.5180\n",
            "Epoch 0, Loss: 0.5026\n",
            "Epoch 0, Loss: 0.5910\n",
            "Epoch 0, Loss: 0.5644\n",
            "Epoch 0, Loss: 0.5859\n",
            "Epoch 0, Loss: 0.5566\n",
            "Epoch 0, Loss: 0.5800\n",
            "Epoch 0, Loss: 0.4421\n",
            "Epoch 0, Loss: 0.5367\n",
            "Epoch 0, Loss: 0.5193\n",
            "Epoch 0, Loss: 0.5772\n",
            "Epoch 0, Loss: 0.5106\n",
            "Epoch 0, Loss: 0.4917\n",
            "Epoch 0, Loss: 0.5846\n",
            "Epoch 0, Loss: 0.4853\n",
            "Epoch 0, Loss: 0.3345\n",
            "Epoch 0, Loss: 0.3036\n",
            "Epoch 0, Loss: 0.2708\n",
            "Epoch 0, Loss: 0.5694\n",
            "Epoch 0, Loss: 0.2335\n",
            "Epoch 0, Loss: 0.2019\n",
            "Epoch 0, Loss: 0.4531\n",
            "Epoch 0, Loss: 0.4333\n",
            "Epoch 0, Loss: 0.4172\n",
            "Epoch 0, Loss: 0.4039\n",
            "Epoch 0, Loss: 0.1874\n",
            "Epoch 0, Loss: 0.3982\n",
            "Epoch 0, Loss: 0.1713\n",
            "Epoch 0, Loss: 0.6192\n",
            "Epoch 0, Loss: 0.4156\n",
            "Epoch 0, Loss: 0.4024\n",
            "Epoch 0, Loss: 0.3912\n",
            "Epoch 0, Loss: 0.6401\n",
            "Epoch 0, Loss: 0.4000\n",
            "Epoch 0, Loss: 0.1625\n",
            "Epoch 0, Loss: 0.6119\n",
            "Epoch 0, Loss: 0.5755\n",
            "Epoch 0, Loss: 0.4395\n",
            "Epoch 0, Loss: 0.4214\n",
            "Epoch 0, Loss: 0.4063\n",
            "Epoch 0, Loss: 0.3940\n",
            "Epoch 0, Loss: 0.3838\n",
            "Epoch 0, Loss: 0.1682\n",
            "Epoch 0, Loss: 0.1482\n",
            "Epoch 0, Loss: 0.6188\n",
            "Epoch 0, Loss: 0.1186\n",
            "Epoch 0, Loss: 0.4223\n",
            "Epoch 0, Loss: 0.1142\n",
            "Epoch 0, Loss: 0.5878\n",
            "Epoch 0, Loss: 0.4370\n",
            "Epoch 0, Loss: 0.5760\n",
            "Epoch 0, Loss: 0.5477\n",
            "Epoch 0, Loss: 0.4588\n",
            "Epoch 0, Loss: 0.1042\n",
            "Epoch 0, Loss: 0.0905\n",
            "Epoch 0, Loss: 0.4559\n",
            "Epoch 0, Loss: 0.0888\n",
            "Epoch 0, Loss: 0.5495\n",
            "Epoch 0, Loss: 0.5249\n",
            "Epoch 0, Loss: 0.4978\n",
            "Epoch 0, Loss: 0.4743\n",
            "Epoch 0, Loss: 0.4502\n",
            "Epoch 0, Loss: 0.4290\n",
            "Epoch 0, Loss: 0.1091\n",
            "Epoch 0, Loss: 0.0947\n",
            "Epoch 0, Loss: 0.4307\n",
            "Epoch 20, Loss: 0.2860\n",
            "Epoch 20, Loss: 0.1647\n",
            "Epoch 20, Loss: 0.2727\n",
            "Epoch 20, Loss: 0.2574\n",
            "Epoch 20, Loss: 0.1771\n",
            "Epoch 20, Loss: 0.2612\n",
            "Epoch 20, Loss: 0.2302\n",
            "Epoch 20, Loss: 0.1990\n",
            "Epoch 20, Loss: 0.2133\n",
            "Epoch 20, Loss: 0.2774\n",
            "Epoch 20, Loss: 0.1967\n",
            "Epoch 20, Loss: 0.2641\n",
            "Epoch 20, Loss: 0.2344\n",
            "Epoch 20, Loss: 0.2078\n",
            "Epoch 20, Loss: 0.2453\n",
            "Epoch 20, Loss: 0.1984\n",
            "Epoch 20, Loss: 0.1737\n",
            "Epoch 20, Loss: 0.2588\n",
            "Epoch 20, Loss: 0.1704\n",
            "Epoch 20, Loss: 0.1474\n",
            "Epoch 20, Loss: 0.2648\n",
            "Epoch 20, Loss: 0.1509\n",
            "Epoch 20, Loss: 0.2942\n",
            "Epoch 20, Loss: 0.2558\n",
            "Epoch 20, Loss: 0.2261\n",
            "Epoch 20, Loss: 0.1823\n",
            "Epoch 20, Loss: 0.2179\n",
            "Epoch 20, Loss: 0.1817\n",
            "Epoch 20, Loss: 0.1578\n",
            "Epoch 20, Loss: 0.2318\n",
            "Epoch 20, Loss: 0.3172\n",
            "Epoch 20, Loss: 0.1685\n",
            "Epoch 20, Loss: 0.2279\n",
            "Epoch 20, Loss: 0.1988\n",
            "Epoch 20, Loss: 0.3122\n",
            "Epoch 20, Loss: 0.1832\n",
            "Epoch 20, Loss: 0.1593\n",
            "Epoch 20, Loss: 0.1381\n",
            "Epoch 20, Loss: 0.3193\n",
            "Epoch 20, Loss: 0.2859\n",
            "Epoch 20, Loss: 0.2558\n",
            "Epoch 20, Loss: 0.1700\n",
            "Epoch 20, Loss: 0.2737\n",
            "Epoch 20, Loss: 0.2489\n",
            "Epoch 20, Loss: 0.2206\n",
            "Epoch 20, Loss: 0.1936\n",
            "Epoch 20, Loss: 0.2620\n",
            "Epoch 20, Loss: 0.1847\n",
            "Epoch 20, Loss: 0.1618\n",
            "Epoch 20, Loss: 0.1419\n",
            "Epoch 20, Loss: 0.2764\n",
            "Epoch 20, Loss: 0.2772\n",
            "Epoch 20, Loss: 0.2500\n",
            "Epoch 20, Loss: 0.2540\n",
            "Epoch 20, Loss: 0.2321\n",
            "Epoch 20, Loss: 0.2022\n",
            "Epoch 20, Loss: 0.2451\n",
            "Epoch 20, Loss: 0.2162\n",
            "Epoch 20, Loss: 0.2468\n",
            "Epoch 20, Loss: 0.2045\n",
            "Epoch 20, Loss: 0.1746\n",
            "Epoch 20, Loss: 0.2537\n",
            "Epoch 20, Loss: 0.1731\n",
            "Epoch 20, Loss: 0.2623\n",
            "Epoch 20, Loss: 0.2353\n",
            "Epoch 20, Loss: 0.2054\n",
            "Epoch 20, Loss: 0.2688\n",
            "Epoch 20, Loss: 0.1948\n",
            "Epoch 20, Loss: 0.1713\n",
            "Epoch 20, Loss: 0.2330\n",
            "Epoch 20, Loss: 0.2036\n",
            "Epoch 20, Loss: 0.1774\n",
            "Epoch 20, Loss: 0.1508\n",
            "Epoch 20, Loss: 0.2255\n",
            "Epoch 20, Loss: 0.1454\n",
            "Epoch 20, Loss: 0.2199\n",
            "Epoch 20, Loss: 0.3308\n",
            "Epoch 20, Loss: 0.1609\n",
            "Epoch 20, Loss: 0.1355\n",
            "Epoch 20, Loss: 0.1143\n",
            "Epoch 20, Loss: 0.3405\n",
            "Epoch 20, Loss: 0.1142\n",
            "Epoch 20, Loss: 0.2656\n",
            "Epoch 20, Loss: 0.3214\n",
            "Epoch 20, Loss: 0.2868\n",
            "Epoch 20, Loss: 0.1491\n",
            "Epoch 20, Loss: 0.1294\n",
            "Epoch 20, Loss: 0.1069\n",
            "Epoch 20, Loss: 0.0933\n",
            "Epoch 20, Loss: 0.0803\n",
            "Epoch 20, Loss: 0.3148\n",
            "Epoch 20, Loss: 0.2775\n",
            "Epoch 20, Loss: 0.3347\n",
            "Epoch 20, Loss: 0.2455\n",
            "Epoch 20, Loss: 0.1348\n",
            "Epoch 20, Loss: 0.2347\n",
            "Epoch 20, Loss: 0.3177\n",
            "Epoch 20, Loss: 0.1534\n",
            "Epoch 20, Loss: 0.2995\n",
            "Epoch 20, Loss: 0.2720\n",
            "Epoch 20, Loss: 0.1635\n",
            "Epoch 20, Loss: 0.2516\n",
            "Epoch 20, Loss: 0.2253\n",
            "Epoch 20, Loss: 0.1694\n",
            "Epoch 20, Loss: 0.2191\n",
            "Epoch 20, Loss: 0.2899\n",
            "Epoch 20, Loss: 0.2631\n",
            "Epoch 20, Loss: 0.2328\n",
            "Epoch 20, Loss: 0.2073\n",
            "Epoch 20, Loss: 0.1814\n",
            "Epoch 20, Loss: 0.1583\n",
            "Epoch 20, Loss: 0.2619\n",
            "Epoch 20, Loss: 0.2353\n",
            "Epoch 20, Loss: 0.1716\n",
            "Epoch 40, Loss: 0.2006\n",
            "Epoch 40, Loss: 0.1660\n",
            "Epoch 40, Loss: 0.1880\n",
            "Epoch 40, Loss: 0.0234\n",
            "Epoch 40, Loss: 0.1891\n",
            "Epoch 40, Loss: 0.1817\n",
            "Epoch 40, Loss: 0.1526\n",
            "Epoch 40, Loss: 0.1270\n",
            "Epoch 40, Loss: 0.2194\n",
            "Epoch 40, Loss: 0.0204\n",
            "Epoch 40, Loss: 0.1848\n",
            "Epoch 40, Loss: 0.0223\n",
            "Epoch 40, Loss: 0.0195\n",
            "Epoch 40, Loss: 0.0181\n",
            "Epoch 40, Loss: 0.1512\n",
            "Epoch 40, Loss: 0.0163\n",
            "Epoch 40, Loss: 0.0151\n",
            "Epoch 40, Loss: 0.1981\n",
            "Epoch 40, Loss: 0.0169\n",
            "Epoch 40, Loss: 0.0154\n",
            "Epoch 40, Loss: 0.1651\n",
            "Epoch 40, Loss: 0.0183\n",
            "Epoch 40, Loss: 0.1718\n",
            "Epoch 40, Loss: 0.1687\n",
            "Epoch 40, Loss: 0.1395\n",
            "Epoch 40, Loss: 0.0201\n",
            "Epoch 40, Loss: 0.1237\n",
            "Epoch 40, Loss: 0.0234\n",
            "Epoch 40, Loss: 0.0214\n",
            "Epoch 40, Loss: 0.1129\n",
            "Epoch 40, Loss: 0.2370\n",
            "Epoch 40, Loss: 0.0200\n",
            "Epoch 40, Loss: 0.1317\n",
            "Epoch 40, Loss: 0.1078\n",
            "Epoch 40, Loss: 0.2563\n",
            "Epoch 40, Loss: 0.1136\n",
            "Epoch 40, Loss: 0.1012\n",
            "Epoch 40, Loss: 0.0852\n",
            "Epoch 40, Loss: 0.2663\n",
            "Epoch 40, Loss: 0.2288\n",
            "Epoch 40, Loss: 0.0244\n",
            "Epoch 40, Loss: 0.1204\n",
            "Epoch 40, Loss: 0.2172\n",
            "Epoch 40, Loss: 0.0221\n",
            "Epoch 40, Loss: 0.0208\n",
            "Epoch 40, Loss: 0.0190\n",
            "Epoch 40, Loss: 0.1700\n",
            "Epoch 40, Loss: 0.0164\n",
            "Epoch 40, Loss: 0.0152\n",
            "Epoch 40, Loss: 0.0145\n",
            "Epoch 40, Loss: 0.1948\n",
            "Epoch 40, Loss: 0.1664\n",
            "Epoch 40, Loss: 0.1652\n",
            "Epoch 40, Loss: 0.1631\n",
            "Epoch 40, Loss: 0.1830\n",
            "Epoch 40, Loss: 0.0173\n",
            "Epoch 40, Loss: 0.1596\n",
            "Epoch 40, Loss: 0.1331\n",
            "Epoch 40, Loss: 0.2104\n",
            "Epoch 40, Loss: 0.1395\n",
            "Epoch 40, Loss: 0.1143\n",
            "Epoch 40, Loss: 0.2074\n",
            "Epoch 40, Loss: 0.1221\n",
            "Epoch 40, Loss: 0.0126\n",
            "Epoch 40, Loss: 0.0119\n",
            "Epoch 40, Loss: 0.0112\n",
            "Epoch 40, Loss: 0.2485\n",
            "Epoch 40, Loss: 0.0127\n",
            "Epoch 40, Loss: 0.0121\n",
            "Epoch 40, Loss: 0.1167\n",
            "Epoch 40, Loss: 0.1013\n",
            "Epoch 40, Loss: 0.0878\n",
            "Epoch 40, Loss: 0.0757\n",
            "Epoch 40, Loss: 0.0092\n",
            "Epoch 40, Loss: 0.0615\n",
            "Epoch 40, Loss: 0.0091\n",
            "Epoch 40, Loss: 0.2967\n",
            "Epoch 40, Loss: 0.0811\n",
            "Epoch 40, Loss: 0.0624\n",
            "Epoch 40, Loss: 0.0544\n",
            "Epoch 40, Loss: 0.2844\n",
            "Epoch 40, Loss: 0.0597\n",
            "Epoch 40, Loss: 0.0085\n",
            "Epoch 40, Loss: 0.2833\n",
            "Epoch 40, Loss: 0.2776\n",
            "Epoch 40, Loss: 0.0759\n",
            "Epoch 40, Loss: 0.0689\n",
            "Epoch 40, Loss: 0.0561\n",
            "Epoch 40, Loss: 0.0501\n",
            "Epoch 40, Loss: 0.0439\n",
            "Epoch 40, Loss: 0.0100\n",
            "Epoch 40, Loss: 0.0091\n",
            "Epoch 40, Loss: 0.3558\n",
            "Epoch 40, Loss: 0.0097\n",
            "Epoch 40, Loss: 0.0535\n",
            "Epoch 40, Loss: 0.0088\n",
            "Epoch 40, Loss: 0.3277\n",
            "Epoch 40, Loss: 0.0751\n",
            "Epoch 40, Loss: 0.2716\n",
            "Epoch 40, Loss: 0.2488\n",
            "Epoch 40, Loss: 0.0941\n",
            "Epoch 40, Loss: 0.0108\n",
            "Epoch 40, Loss: 0.0105\n",
            "Epoch 40, Loss: 0.0764\n",
            "Epoch 40, Loss: 0.0099\n",
            "Epoch 40, Loss: 0.2538\n",
            "Epoch 40, Loss: 0.2575\n",
            "Epoch 40, Loss: 0.2420\n",
            "Epoch 40, Loss: 0.2103\n",
            "Epoch 40, Loss: 0.1627\n",
            "Epoch 40, Loss: 0.1604\n",
            "Epoch 40, Loss: 0.0223\n",
            "Epoch 40, Loss: 0.0214\n",
            "Epoch 40, Loss: 0.1398\n",
            "Epoch 60, Loss: 0.1752\n",
            "Epoch 60, Loss: 0.1269\n",
            "Epoch 60, Loss: 0.1500\n",
            "Epoch 60, Loss: 0.0289\n",
            "Epoch 60, Loss: 0.2763\n",
            "Epoch 60, Loss: 0.1294\n",
            "Epoch 60, Loss: 0.1087\n",
            "Epoch 60, Loss: 0.0883\n",
            "Epoch 60, Loss: 0.1598\n",
            "Epoch 60, Loss: 0.0245\n",
            "Epoch 60, Loss: 0.1455\n",
            "Epoch 60, Loss: 0.0279\n",
            "Epoch 60, Loss: 0.0241\n",
            "Epoch 60, Loss: 0.0222\n",
            "Epoch 60, Loss: 0.1025\n",
            "Epoch 60, Loss: 0.0190\n",
            "Epoch 60, Loss: 0.0174\n",
            "Epoch 60, Loss: 0.1635\n",
            "Epoch 60, Loss: 0.0202\n",
            "Epoch 60, Loss: 0.0184\n",
            "Epoch 60, Loss: 0.1408\n",
            "Epoch 60, Loss: 0.0223\n",
            "Epoch 60, Loss: 0.1072\n",
            "Epoch 60, Loss: 0.1409\n",
            "Epoch 60, Loss: 0.1230\n",
            "Epoch 60, Loss: 0.0256\n",
            "Epoch 60, Loss: 0.1109\n",
            "Epoch 60, Loss: 0.0301\n",
            "Epoch 60, Loss: 0.0270\n",
            "Epoch 60, Loss: 0.1081\n",
            "Epoch 60, Loss: 0.1797\n",
            "Epoch 60, Loss: 0.0249\n",
            "Epoch 60, Loss: 0.1822\n",
            "Epoch 60, Loss: 0.1004\n",
            "Epoch 60, Loss: 0.2724\n",
            "Epoch 60, Loss: 0.1240\n",
            "Epoch 60, Loss: 0.0906\n",
            "Epoch 60, Loss: 0.0771\n",
            "Epoch 60, Loss: 0.1758\n",
            "Epoch 60, Loss: 0.1538\n",
            "Epoch 60, Loss: 0.0307\n",
            "Epoch 60, Loss: 0.1049\n",
            "Epoch 60, Loss: 0.1553\n",
            "Epoch 60, Loss: 0.0276\n",
            "Epoch 60, Loss: 0.0256\n",
            "Epoch 60, Loss: 0.0231\n",
            "Epoch 60, Loss: 0.1028\n",
            "Epoch 60, Loss: 0.0192\n",
            "Epoch 60, Loss: 0.0177\n",
            "Epoch 60, Loss: 0.0167\n",
            "Epoch 60, Loss: 0.2734\n",
            "Epoch 60, Loss: 0.1035\n",
            "Epoch 60, Loss: 0.1439\n",
            "Epoch 60, Loss: 0.1243\n",
            "Epoch 60, Loss: 0.2236\n",
            "Epoch 60, Loss: 0.0213\n",
            "Epoch 60, Loss: 0.1043\n",
            "Epoch 60, Loss: 0.0886\n",
            "Epoch 60, Loss: 0.1911\n",
            "Epoch 60, Loss: 0.1093\n",
            "Epoch 60, Loss: 0.0767\n",
            "Epoch 60, Loss: 0.1673\n",
            "Epoch 60, Loss: 0.0985\n",
            "Epoch 60, Loss: 0.0150\n",
            "Epoch 60, Loss: 0.0141\n",
            "Epoch 60, Loss: 0.0132\n",
            "Epoch 60, Loss: 0.2941\n",
            "Epoch 60, Loss: 0.0155\n",
            "Epoch 60, Loss: 0.0147\n",
            "Epoch 60, Loss: 0.0769\n",
            "Epoch 60, Loss: 0.0718\n",
            "Epoch 60, Loss: 0.0789\n",
            "Epoch 60, Loss: 0.0531\n",
            "Epoch 60, Loss: 0.0098\n",
            "Epoch 60, Loss: 0.0394\n",
            "Epoch 60, Loss: 0.0093\n",
            "Epoch 60, Loss: 0.2643\n",
            "Epoch 60, Loss: 0.0917\n",
            "Epoch 60, Loss: 0.0412\n",
            "Epoch 60, Loss: 0.0362\n",
            "Epoch 60, Loss: 0.2243\n",
            "Epoch 60, Loss: 0.0402\n",
            "Epoch 60, Loss: 0.0089\n",
            "Epoch 60, Loss: 0.2278\n",
            "Epoch 60, Loss: 0.2158\n",
            "Epoch 60, Loss: 0.0566\n",
            "Epoch 60, Loss: 0.0618\n",
            "Epoch 60, Loss: 0.0396\n",
            "Epoch 60, Loss: 0.0360\n",
            "Epoch 60, Loss: 0.0313\n",
            "Epoch 60, Loss: 0.0092\n",
            "Epoch 60, Loss: 0.0086\n",
            "Epoch 60, Loss: 0.3330\n",
            "Epoch 60, Loss: 0.0098\n",
            "Epoch 60, Loss: 0.0417\n",
            "Epoch 60, Loss: 0.0088\n",
            "Epoch 60, Loss: 0.2377\n",
            "Epoch 60, Loss: 0.1583\n",
            "Epoch 60, Loss: 0.2136\n",
            "Epoch 60, Loss: 0.1928\n",
            "Epoch 60, Loss: 0.0790\n",
            "Epoch 60, Loss: 0.0127\n",
            "Epoch 60, Loss: 0.0122\n",
            "Epoch 60, Loss: 0.0525\n",
            "Epoch 60, Loss: 0.0110\n",
            "Epoch 60, Loss: 0.2005\n",
            "Epoch 60, Loss: 0.2329\n",
            "Epoch 60, Loss: 0.1969\n",
            "Epoch 60, Loss: 0.1379\n",
            "Epoch 60, Loss: 0.1226\n",
            "Epoch 60, Loss: 0.1160\n",
            "Epoch 60, Loss: 0.0319\n",
            "Epoch 60, Loss: 0.0298\n",
            "Epoch 60, Loss: 0.1088\n",
            "Epoch 80, Loss: 0.1805\n",
            "Epoch 80, Loss: 0.0481\n",
            "Epoch 80, Loss: 0.0498\n",
            "Epoch 80, Loss: 0.0142\n",
            "Epoch 80, Loss: 0.4284\n",
            "Epoch 80, Loss: 0.0436\n",
            "Epoch 80, Loss: 0.0384\n",
            "Epoch 80, Loss: 0.0317\n",
            "Epoch 80, Loss: 0.0517\n",
            "Epoch 80, Loss: 0.0155\n",
            "Epoch 80, Loss: 0.0687\n",
            "Epoch 80, Loss: 0.0170\n",
            "Epoch 80, Loss: 0.0144\n",
            "Epoch 80, Loss: 0.0132\n",
            "Epoch 80, Loss: 0.0367\n",
            "Epoch 80, Loss: 0.0124\n",
            "Epoch 80, Loss: 0.0113\n",
            "Epoch 80, Loss: 0.0613\n",
            "Epoch 80, Loss: 0.0126\n",
            "Epoch 80, Loss: 0.0113\n",
            "Epoch 80, Loss: 0.0486\n",
            "Epoch 80, Loss: 0.0132\n",
            "Epoch 80, Loss: 0.0382\n",
            "Epoch 80, Loss: 0.0413\n",
            "Epoch 80, Loss: 0.0373\n",
            "Epoch 80, Loss: 0.0136\n",
            "Epoch 80, Loss: 0.0337\n",
            "Epoch 80, Loss: 0.0155\n",
            "Epoch 80, Loss: 0.0138\n",
            "Epoch 80, Loss: 0.0379\n",
            "Epoch 80, Loss: 0.1021\n",
            "Epoch 80, Loss: 0.0122\n",
            "Epoch 80, Loss: 0.2280\n",
            "Epoch 80, Loss: 0.0293\n",
            "Epoch 80, Loss: 0.4290\n",
            "Epoch 80, Loss: 0.0363\n",
            "Epoch 80, Loss: 0.0322\n",
            "Epoch 80, Loss: 0.0293\n",
            "Epoch 80, Loss: 0.0590\n",
            "Epoch 80, Loss: 0.0599\n",
            "Epoch 80, Loss: 0.0138\n",
            "Epoch 80, Loss: 0.0379\n",
            "Epoch 80, Loss: 0.0828\n",
            "Epoch 80, Loss: 0.0124\n",
            "Epoch 80, Loss: 0.0118\n",
            "Epoch 80, Loss: 0.0106\n",
            "Epoch 80, Loss: 0.0274\n",
            "Epoch 80, Loss: 0.0103\n",
            "Epoch 80, Loss: 0.0094\n",
            "Epoch 80, Loss: 0.0089\n",
            "Epoch 80, Loss: 0.3969\n",
            "Epoch 80, Loss: 0.0357\n",
            "Epoch 80, Loss: 0.0445\n",
            "Epoch 80, Loss: 0.1199\n",
            "Epoch 80, Loss: 0.3660\n",
            "Epoch 80, Loss: 0.0123\n",
            "Epoch 80, Loss: 0.0349\n",
            "Epoch 80, Loss: 0.0312\n",
            "Epoch 80, Loss: 0.1673\n",
            "Epoch 80, Loss: 0.0800\n",
            "Epoch 80, Loss: 0.0329\n",
            "Epoch 80, Loss: 0.0502\n",
            "Epoch 80, Loss: 0.1014\n",
            "Epoch 80, Loss: 0.0099\n",
            "Epoch 80, Loss: 0.0092\n",
            "Epoch 80, Loss: 0.0086\n",
            "Epoch 80, Loss: 0.3966\n",
            "Epoch 80, Loss: 0.0104\n",
            "Epoch 80, Loss: 0.0098\n",
            "Epoch 80, Loss: 0.0312\n",
            "Epoch 80, Loss: 0.0320\n",
            "Epoch 80, Loss: 0.0506\n",
            "Epoch 80, Loss: 0.0285\n",
            "Epoch 80, Loss: 0.0075\n",
            "Epoch 80, Loss: 0.0183\n",
            "Epoch 80, Loss: 0.0073\n",
            "Epoch 80, Loss: 0.1069\n",
            "Epoch 80, Loss: 0.0777\n",
            "Epoch 80, Loss: 0.0180\n",
            "Epoch 80, Loss: 0.0166\n",
            "Epoch 80, Loss: 0.0780\n",
            "Epoch 80, Loss: 0.0185\n",
            "Epoch 80, Loss: 0.0075\n",
            "Epoch 80, Loss: 0.0921\n",
            "Epoch 80, Loss: 0.0634\n",
            "Epoch 80, Loss: 0.0278\n",
            "Epoch 80, Loss: 0.0494\n",
            "Epoch 80, Loss: 0.0188\n",
            "Epoch 80, Loss: 0.0178\n",
            "Epoch 80, Loss: 0.0160\n",
            "Epoch 80, Loss: 0.0079\n",
            "Epoch 80, Loss: 0.0073\n",
            "Epoch 80, Loss: 0.2513\n",
            "Epoch 80, Loss: 0.0089\n",
            "Epoch 80, Loss: 0.0316\n",
            "Epoch 80, Loss: 0.0078\n",
            "Epoch 80, Loss: 0.0566\n",
            "Epoch 80, Loss: 0.3304\n",
            "Epoch 80, Loss: 0.0647\n",
            "Epoch 80, Loss: 0.0595\n",
            "Epoch 80, Loss: 0.0383\n",
            "Epoch 80, Loss: 0.0097\n",
            "Epoch 80, Loss: 0.0092\n",
            "Epoch 80, Loss: 0.0233\n",
            "Epoch 80, Loss: 0.0087\n",
            "Epoch 80, Loss: 0.0635\n",
            "Epoch 80, Loss: 0.1421\n",
            "Epoch 80, Loss: 0.0469\n",
            "Epoch 80, Loss: 0.0342\n",
            "Epoch 80, Loss: 0.0326\n",
            "Epoch 80, Loss: 0.0291\n",
            "Epoch 80, Loss: 0.0155\n",
            "Epoch 80, Loss: 0.0150\n",
            "Epoch 80, Loss: 0.0308\n"
          ]
        }
      ]
    }
  ]
}